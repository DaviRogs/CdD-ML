{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa85aa9",
   "metadata": {},
   "source": [
    "# Tópico 3 – Validação cruzada + Análise de Erros\n",
    "\n",
    "Este notebook aplica validação cruzada e análise de erros utilizando dados da API da Câmara dos Deputados para prever se proposições serão aprovadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 1) Defina o caminho correto para o df_consolidado.csv\n",
    "# ------------------------------------------------\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "csv_path = os.path.join(BASE_DIR, \"dados\", \"df_consolidado.csv\")\n",
    "df = pd.read_csv(csv_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd30fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 2) Elimine todas as linhas que tenham NaN\n",
    "#    em QUALQUER coluna que vamos usar como feature ou target\n",
    "# ------------------------------------------------\n",
    "colunas_para_validar = [\n",
    "    \"tipoVoto\",\n",
    "    \"siglaUf\",\n",
    "    \"id_partido\",\n",
    "    \"cod_tipo\",\n",
    "    \"numero_proposicao\",\n",
    "    \"ano\",\n",
    "    \"tema\",\n",
    "    \"aprovacao\"\n",
    "]\n",
    "df = df.dropna(subset=colunas_para_validar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 3) Converta a coluna tipoVoto para binário (1/0)\n",
    "# ------------------------------------------------\n",
    "df[\"tipoVoto\"] = df[\"tipoVoto\"].map({\"Sim\": 1, \"Não\": 0})\n",
    "# (caso haja valores diferentes de “Sim” ou “Não”, eles ficam NaN e já foram descartados pelo dropna acima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Defina X (features) e y (target) – sem NaNs agora\n",
    "# ------------------------------------------------\n",
    "y = df[\"aprovacao\"]\n",
    "\n",
    "features = [\n",
    "    \"siglaUf\",\n",
    "    \"id_partido\",\n",
    "    \"cod_tipo\",\n",
    "    \"numero_proposicao\",\n",
    "    \"ano\",\n",
    "    \"tema\"\n",
    "]\n",
    "X = df[features].copy()\n",
    "\n",
    "# “tema” é, na prática, uma lista/array de string. Vamos extrair o primeiro tema\n",
    "# para simplificar em “tema_principal”:\n",
    "def extrai_primeiro_tema(x):\n",
    "    try:\n",
    "        # se x for algo como \"['Saúde', 'Educação']\", retorna \"Saúde\"\n",
    "        return eval(x)[0]\n",
    "    except Exception:\n",
    "        return \"Outros\"\n",
    "\n",
    "X[\"tema_principal\"] = X[\"tema\"].apply(extrai_primeiro_tema)\n",
    "\n",
    "# Selecionamos somente as colunas que vamos usar\n",
    "X_final = X[[\n",
    "    \"siglaUf\",\n",
    "    \"id_partido\",\n",
    "    \"cod_tipo\",\n",
    "    \"ano\",\n",
    "    \"tema_principal\"\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6051220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Novamente, garantimos que não haja NENHUM NaN em X_final ou em y\n",
    "# ------------------------------------------------\n",
    "mask = X_final.notna().all(axis=1) & y.notna()\n",
    "X_final = X_final.loc[mask]\n",
    "y       = y.loc[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738dd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 6) Construímos um pipeline que faz o OneHotEncoder\n",
    "#    nas colunas categóricas e deixa as numéricas “pass through”\n",
    "# ------------------------------------------------\n",
    "categorical_features = [\"siglaUf\", \"tema_principal\"]\n",
    "numeric_features     = [\"id_partido\", \"cod_tipo\", \"ano\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"  # opcional, mas recomendado se as classes estiverem desbalanceadas\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 7) Agora sim, vamos rodar o cross_val_score usando o pipeline em vez do RandomForest puro\n",
    "# ------------------------------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    pipeline,\n",
    "    X_final,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"F1-macro em cada fold:\", scores)\n",
    "print(\"Média F1-macro:\", scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 8) Se quiser também avaliar a Matriz de Confusão no conjunto de teste:\n",
    "# ------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Matriz de Confusão (Test Set)\")\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
