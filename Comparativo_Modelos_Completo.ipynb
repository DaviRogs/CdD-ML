{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6274343",
   "metadata": {},
   "source": [
    "# üìä Compara√ß√£o de Modelos de Classifica√ß√£o - Aprova√ß√£o de Proposi√ß√µes\n",
    "\n",
    "Este notebook consolida os c√≥digos e resultados dos seguintes modelos treinados com base nos dados de proposi√ß√µes legislativas:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "As m√©tricas utilizadas incluem **acur√°cia**, **precis√£o**, **recall** e **F1-score**, al√©m de an√°lise de **import√¢ncia de features** nos modelos baseados em √°rvore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356be4b2",
   "metadata": {},
   "source": [
    "## üìã Tabela Comparativa\n",
    "\n",
    "| Modelo                 | Acur√°cia | Precision (classe 1) | Recall (classe 1) | F1-Score (classe 1) | Observa√ß√µes                                  |\n",
    "|------------------------|----------|------------------------|--------------------|----------------------|----------------------------------------------|\n",
    "| **Random Forest**      | 0.93     | ~0.95                  | ~0.98              | ~0.96                | Feature importance plot inclu√≠do             |\n",
    "| **Decision Tree**      | 0.91     | ~0.92                  | ~0.97              | ~0.94                | Feature importance plot inclu√≠do             |\n",
    "| **K-Nearest Neighbors**| 0.89     | ~0.90                  | ~0.95              | ~0.92                | Usou `StandardScaler` no pipeline            |\n",
    "| **SVM (RBF kernel)**   | 0.90     | ~0.91                  | ~0.96              | ~0.93                | Treinado no mesmo script do KNN              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a38dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# M√©tricas (valores aproximados)\n",
    "modelos = [\"Random Forest\", \"Decision Tree\", \"KNN\", \"SVM\"]\n",
    "acuracias = [0.93, 0.91, 0.89, 0.90]\n",
    "f1_scores = [0.96, 0.94, 0.92, 0.93]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(modelos, acuracias, color='skyblue', label='Acur√°cia')\n",
    "plt.bar(modelos, f1_scores, color='orange', alpha=0.7, label='F1-Score')\n",
    "plt.title(\"Compara√ß√£o de Modelos - Acur√°cia e F1-Score\")\n",
    "plt.ylabel(\"Pontua√ß√£o\")\n",
    "plt.legend()\n",
    "plt.ylim(0.85, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873e98f",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclus√£o\n",
    "\n",
    "- O modelo **Random Forest** teve o melhor desempenho geral, com maior acur√°cia e F1-score.\n",
    "- Modelos baseados em √°rvore (**Decision Tree** e **Random Forest**) tamb√©m permitem interpretar melhor as features importantes.\n",
    "- **KNN** e **SVM** apresentaram desempenho competitivo, mas com custo computacional maior, especialmente para grandes volumes de dados.\n",
    "\n",
    "### üîÅ Pr√≥ximos passos sugeridos:\n",
    "- Testar outros hiperpar√¢metros com `GridSearchCV`\n",
    "- Considerar balanceamento adicional das classes ou t√©cnicas de oversampling\n",
    "- Salvar e versionar os modelos via `joblib`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edca904",
   "metadata": {},
   "source": [
    "## üíª C√≥digo - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import joblib as jb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv('../dados/df_consolidado.csv', dtype={\n",
    "    'id_votacao': 'str',\n",
    "    'id_deputado': 'int64',\n",
    "    'tipoVoto': 'str',\n",
    "    'siglaUf': 'str',\n",
    "    'id_partido': 'float64',\n",
    "    'id_proposicao': 'int64',\n",
    "    'data': 'str',\n",
    "    'sigla_orgao': 'str',\n",
    "    'aprovacao': 'float64',\n",
    "    'cod_tipo': 'int64',\n",
    "    'numero_proposicao': 'int64',\n",
    "    'ano': 'int64',\n",
    "    'orientacao': 'str',\n",
    "    'id_autor': 'float64',\n",
    "    'tema': 'str'\n",
    "})\n",
    "\n",
    "# Remover linhas onde qualquer uma das colunas especificadas √© NaN\n",
    "colunas_para_remover_nan = [\n",
    "    'id_votacao', 'id_deputado', 'tipoVoto', 'siglaUf', 'id_partido',\n",
    "    'id_proposicao', 'data', 'sigla_orgao', 'aprovacao', 'cod_tipo',\n",
    "    'numero_proposicao', 'ano', 'orientacao', 'id_autor', 'tema'\n",
    "]\n",
    "df = df.dropna(subset=colunas_para_remover_nan)\n",
    "\n",
    "# Pr√©-processamento\n",
    "# Converter voto em vari√°vel bin√°ria (1 = a favor, 0 = contra/absten√ß√£o)\n",
    "df['voto_favoravel'] = df['tipoVoto'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "\n",
    "# Criar vari√°vel alvo: aprova√ß√£o da proposi√ß√£o\n",
    "# (assumindo que aprovacao=1.0 significa aprovada)\n",
    "y = df['aprovacao']\n",
    "\n",
    "# Selecionar features relevantes\n",
    "features = ['siglaUf', 'id_partido', 'cod_tipo', 'numero_proposicao', 'ano', 'tema']\n",
    "X = df[features].copy()\n",
    "\n",
    "# Pr√©-processamento das features categ√≥ricas\n",
    "# Para temas, vamos extrair os principais temas (primeiro da lista)\n",
    "X.loc[:, 'tema_principal'] = X['tema'].apply(lambda x: eval(x)[0] if pd.notnull(x) else 'Outros')\n",
    "\n",
    "# Selecionar colunas finais para o modelo\n",
    "final_features = ['siglaUf', 'id_partido', 'cod_tipo', 'ano', 'tema_principal']\n",
    "X_final = X[final_features]\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "categorical_features = ['siglaUf', 'tema_principal']\n",
    "numeric_features = ['id_partido', 'cod_tipo', 'ano']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calcular pesos das classes\n",
    "classes = np.array([0.0, 1.0])  # Converta para array NumPy\n",
    "weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Criar pipeline com pr√©-processamento e modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=42,\n",
    "        class_weight=class_weights\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Acur√°cia: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Obter as import√¢ncias das features\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = numeric_features + list(pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features))\n",
    "\n",
    "# Criar o gr√°fico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances, color='skyblue')\n",
    "plt.xlabel('Import√¢ncia')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Import√¢ncia das Features')\n",
    "plt.gca().invert_yaxis()  # Inverter o eixo para exibir a feature mais importante no topo\n",
    "plt.show()\n",
    "\n",
    "# Salvar png, caso necess√°rio\n",
    "# plt.savefig('feature_importance_DecisionTree.png', dpi=300)\n",
    "\n",
    "# Salvar modelo para uso futuro\n",
    "# jb.dump(pipeline, 'modelo_aprovacao_proposicao.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c7c29",
   "metadata": {},
   "source": [
    "## üíª C√≥digo - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import joblib as jb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Carregar os dados (mantendo seu c√≥digo original)\n",
    "df = pd.read_csv('../dados/df_consolidado.csv', dtype={\n",
    "    'id_votacao': 'str',\n",
    "    'id_deputado': 'int64',\n",
    "    'tipoVoto': 'str',\n",
    "    'siglaUf': 'str',\n",
    "    'id_partido': 'float64',\n",
    "    'id_proposicao': 'int64',\n",
    "    'data': 'str',\n",
    "    'sigla_orgao': 'str',\n",
    "    'aprovacao': 'float64',\n",
    "    'cod_tipo': 'int64',\n",
    "    'numero_proposicao': 'int64',\n",
    "    'ano': 'int64',\n",
    "    'orientacao': 'str',\n",
    "    'id_autor': 'float64',\n",
    "    'tema': 'str'\n",
    "})\n",
    "\n",
    "# Remover linhas onde qualquer uma das colunas especificadas √© NaN\n",
    "colunas_para_remover_nan = [\n",
    "    'id_votacao', 'id_deputado', 'tipoVoto', 'siglaUf', 'id_partido',\n",
    "    'id_proposicao', 'data', 'sigla_orgao', 'aprovacao', 'cod_tipo',\n",
    "    'numero_proposicao', 'ano', 'orientacao', 'id_autor', 'tema'\n",
    "]\n",
    "df = df.dropna(subset=colunas_para_remover_nan)\n",
    "\n",
    "# Pr√©-processamento\n",
    "# Converter voto em vari√°vel bin√°ria (1 = a favor, 0 = contra/absten√ß√£o)\n",
    "df['voto_favoravel'] = df['tipoVoto'].apply(lambda x: 1 if x == '1.0' else 0)\n",
    "\n",
    "# Criar vari√°vel alvo: aprova√ß√£o da proposi√ß√£o\n",
    "y = df['aprovacao']\n",
    "\n",
    "# Selecionar features relevantes\n",
    "features = ['siglaUf', 'id_partido', 'cod_tipo', 'numero_proposicao', 'ano', 'tema']\n",
    "X = df[features].copy()\n",
    "\n",
    "# Pr√©-processamento das features categ√≥ricas\n",
    "# Para temas, vamos extrair os principais temas (primeiro da lista)\n",
    "X.loc[:, 'tema_principal'] = X['tema'].apply(lambda x: eval(x)[0] if pd.notnull(x) else 'Outros')\n",
    "\n",
    "# Selecionar colunas finais para o modelo\n",
    "final_features = ['siglaUf', 'id_partido', 'cod_tipo', 'ano', 'tema_principal']\n",
    "X_final = X[final_features]\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "categorical_features = ['siglaUf', 'tema_principal']\n",
    "numeric_features = ['id_partido', 'cod_tipo', 'ano']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calcular pesos das classes para lidar com desbalanceamento\n",
    "classes = np.unique(y_train)\n",
    "weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Criar pipeline com pr√©-processamento e modelo Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,  # N√∫mero de √°rvores na floresta\n",
    "        max_depth=15,      # Profundidade m√°xima das √°rvores\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight=class_weights,\n",
    "        n_jobs=-1         # Usar todos os cores dispon√≠veis\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Opcional: Otimiza√ß√£o de hiperpar√¢metros com GridSearchCV\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [50, 100, 200],\n",
    "#     'classifier__max_depth': [10, 15, 20, None],\n",
    "#     'classifier__min_samples_split': [2, 5, 10],\n",
    "#     'classifier__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Treinar modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Acur√°cia: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importance (ap√≥s o pr√©-processamento)\n",
    "# Precisamos obter os nomes das features ap√≥s o OneHotEncoding\n",
    "encoder = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "encoded_cat_features = encoder.get_feature_names_out(categorical_features)\n",
    "all_features = numeric_features + list(encoded_cat_features)\n",
    "\n",
    "# Obter import√¢ncia das features\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plotar a import√¢ncia das features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Import√¢ncia')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Import√¢ncia das Features')\n",
    "plt.gca().invert_yaxis()  # Inverter o eixo para exibir a feature mais importante no topo\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Salvar png, caso necess√°rio\n",
    "# plt.savefig('feature_importance_RandomForest.png', dpi=300)\n",
    "\n",
    "# Salvar modelo para uso futuro\n",
    "# jb.dump(pipeline, 'modelo_random_forest_aprovacao.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d65ce",
   "metadata": {},
   "source": [
    "## üíª C√≥digo - KNN e SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ef8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "caminho_base = os.path.dirname(os.path.abspath(__file__))\n",
    "caminho_csv = os.path.join(caminho_base, '..', 'dados', 'df_consolidado.csv')\n",
    "\n",
    "df = pd.read_csv(caminho_csv,low_memory=False)\n",
    "\n",
    "# Vari√°vel alvo\n",
    "df['voto_favoravel'] = df['tipoVoto'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "y = df['aprovacao']\n",
    "\n",
    "# Sele√ß√£o de features\n",
    "features = ['siglaUf', 'id_partido', 'cod_tipo', 'numero_proposicao', 'ano', 'tema']\n",
    "X = df[features]\n",
    "X = X.copy()\n",
    "X['tema_principal'] = X['tema'].apply(lambda x: eval(x)[0] if pd.notnull(x) else 'Outros')\n",
    "X_final = X[['siglaUf', 'id_partido', 'cod_tipo', 'ano', 'tema_principal']]\n",
    "dados = pd.concat([X_final, y], axis=1).dropna()\n",
    "X_final = dados[X_final.columns]\n",
    "y = dados['aprovacao']\n",
    "\n",
    "# Divis√£o das colunas\n",
    "categorical_features = ['siglaUf', 'tema_principal']\n",
    "numeric_features = ['id_partido', 'cod_tipo', 'ano']\n",
    "\n",
    "# Pr√©-processamento com escalonamento\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Divis√£o dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fun√ß√£o para treinar e avaliar um modelo\n",
    "def avaliar_modelo(modelo, nome_modelo):\n",
    "    print(f\"\\nüîç Avaliando modelo: {nome_modelo}\")\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', modelo)\n",
    "    ])\n",
    "    \n",
    "    inicio = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    tempo_treino = time.time() - inicio\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"‚è± Tempo de treino e predi√ß√£o: {tempo_treino:.2f} segundos\")\n",
    "    print(f\"‚úÖ Acur√°cia: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(\"üìä Relat√≥rio de Classifica√ß√£o:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Avaliar KNN\n",
    "knn_model = avaliar_modelo(KNeighborsClassifier(n_neighbors=5), \"K-Nearest Neighbors\")\n",
    "\n",
    "# Avaliar SVM\n",
    "svm_model = avaliar_modelo(SVC(kernel='rbf', C=1.0), \"Support Vector Machine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42556e9",
   "metadata": {},
   "source": [
    "## üíª C√≥digo - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acdfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import joblib as jb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv('../dados/df_consolidado.csv', dtype={\n",
    "    'id_votacao': 'str',\n",
    "    'id_deputado': 'int64',\n",
    "    'tipoVoto': 'str',\n",
    "    'siglaUf': 'str',\n",
    "    'id_partido': 'float64',\n",
    "    'id_proposicao': 'int64',\n",
    "    'data': 'str',\n",
    "    'sigla_orgao': 'str',\n",
    "    'aprovacao': 'float64',\n",
    "    'cod_tipo': 'int64',\n",
    "    'numero_proposicao': 'int64',\n",
    "    'ano': 'int64',\n",
    "    'orientacao': 'str',\n",
    "    'id_autor': 'float64',\n",
    "    'tema': 'str'\n",
    "})\n",
    "\n",
    "# Remover linhas onde qualquer uma das colunas especificadas √© NaN\n",
    "colunas_para_remover_nan = [\n",
    "    'id_votacao', 'id_deputado', 'tipoVoto', 'siglaUf', 'id_partido',\n",
    "    'id_proposicao', 'data', 'sigla_orgao', 'aprovacao', 'cod_tipo',\n",
    "    'numero_proposicao', 'ano', 'orientacao', 'id_autor', 'tema'\n",
    "]\n",
    "df = df.dropna(subset=colunas_para_remover_nan)\n",
    "\n",
    "# Pr√©-processamento\n",
    "# Converter voto em vari√°vel bin√°ria (1 = a favor, 0 = contra/absten√ß√£o)\n",
    "df['voto_favoravel'] = df['tipoVoto'].apply(lambda x: 1 if x == '1.0' else 0)\n",
    "\n",
    "# Criar vari√°vel alvo: aprova√ß√£o da proposi√ß√£o\n",
    "# (assumindo que aprovacao=1.0 significa aprovada)\n",
    "y = df['aprovacao']\n",
    "\n",
    "# Selecionar features relevantes\n",
    "features = ['siglaUf', 'id_partido', 'cod_tipo', 'numero_proposicao', 'ano', 'tema']\n",
    "X = df[features].copy()\n",
    "\n",
    "# Pr√©-processamento das features categ√≥ricas\n",
    "# Para temas, vamos extrair os principais temas (primeiro da lista)\n",
    "X.loc[:, 'tema_principal'] = X['tema'].apply(lambda x: eval(x)[0] if pd.notnull(x) else 'Outros')\n",
    "\n",
    "# Selecionar colunas finais para o modelo\n",
    "final_features = ['siglaUf', 'id_partido', 'cod_tipo', 'ano', 'tema_principal']\n",
    "X_final = X[final_features]\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "categorical_features = ['siglaUf', 'tema_principal']\n",
    "numeric_features = ['id_partido', 'cod_tipo', 'ano']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Calcular balanceamento de classes\n",
    "class_weights = class_weight.compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Op√ß√£o 1: Gradient Boosting do sklearn\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        subsample=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Op√ß√£o 2: XGBoost (descomentar para usar)\n",
    "\"\"\"\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=np.sum(y_train == 0) / np.sum(y_train == 1)  # Para dados desbalanceados\n",
    "    ))\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# Treinar modelo\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=class_weights)\n",
    "\n",
    "# Avaliar modelo\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]  # Probabilidades para classe positiva\n",
    "\n",
    "print(f\"Acur√°cia: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_proba):.2f}\")\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Feature importance (para XGBoost ou GradientBoosting)\n",
    "# try:\n",
    "#     importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "#     feature_names = (numeric_features + \n",
    "#                     list(pipeline.named_steps['preprocessor']\n",
    "#                         .named_transformers_['cat']\n",
    "#                         .get_feature_names_out(categorical_features)))\n",
    "    \n",
    "#     importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "#     importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "#     print(\"\\nImport√¢ncia das Features:\")\n",
    "#     print(importance_df.head(20))\n",
    "# except Exception as e:\n",
    "#     print(\"\\nN√£o foi poss√≠vel extrair import√¢ncia das features:\", str(e))\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Import√¢ncia')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Import√¢ncia das Features')\n",
    "    plt.gca().invert_yaxis()  # Inverter o eixo para exibir a feature mais importante no topo\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"\\nN√£o foi poss√≠vel plotar a import√¢ncia das features:\", str(e))\n",
    "\n",
    "# Salvar png, caso necess√°rio\n",
    "# plt.savefig('feature_importance_xgboost.png', dpi=300)\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "# jb.dump(pipeline, 'modelo_aprovacao_gb.pkl')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
